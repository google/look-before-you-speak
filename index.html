<DOCTYPE html>
    <title> Look Before you Speak: Visually Contextualized Utterances</title>

    <meta charset="utf-8">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Project Page for Look Before you Speak">
     <meta name="author" content="Ruilong">

    <meta property="og:url" content="https://google.github.io/aichoreographer/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Look Before you Speak: Visually Contextualized Utterances">
    <meta property="og:description" content="Look Before you Speak: Visually Contextualized Utterances">


<!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-JM2CPK6QLP"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-JM2CPK6QLP');
    </script>
</head>
    <style>
        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fCBc4EsA.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBxc4EsA.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fChc4EsA.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBBc4.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7mxKOzY.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4WxKOzY.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7GxKOzY.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4mxK.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }
    </style>

    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <body>
        <div class="container">
            <div class="row mb-2 mt-4" id="paper-title">
                <h1 class="col-md-12 text-center">
                    Look Before you Speak: Visually Contextualized Utterances
                </h1>
                <h3 class="col-md-12 text-center">
                </h3>
            </div>

            <div class="row" id="authors">
                <div class="mx-auto text-center">
                    <ul class="list-inline mb-0">
                        <li class="list-inline-item">
                            <a href="https://phseo.github.io/">Paul Hongsuck Seo</a>

                        <li class="list-inline-item">
                            <a href="https://a-nagrani.github.io/"> Arsha Nagrani</a>

                        <li class="list-inline-item">
                            <a href="http://lear.inrialpes.fr/people/schmid/">Cordelia Schmid</a>
                    </ul>
                    <p id="institution">
                        Google Research
                    </p>
                </div>
            </div>
            <div class="row mb-2" id="links">
                <div class="mx-auto">
                    <ul class="nav">
                        <li class="nav-item text-center">
                            <a href="https://arxiv.org/abs/2012.05710" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                                </svg><br>
                                Paper
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">
                    <div id="dynamic-teaser">

                        <div id="teaser-dtu" class="pt-3 pb-4">
                            <div class="row no-gutters">
                                <div class="col-12">
                                    <span class="helper"></span><img src="assets/teaser.png" class="img-responsive">
                                </div>
                            </div> 
                        </div>
                    </div> <!-- dynamic-teaser -->
                    <p class="text-justify">
                        <strong>Visually Contextualised Future Utterance Prediction.</strong> 
                        Given an instructional video with paired text and video data, we predict the next 
                        utterance in the video using a Co-attentional Multimodal Video Transformer.  
                        Our model trained on this task also achieves state-of-the-art performance on 
                        downstream VideoQA benchmarks.
                    </p>

                </div>
            </div>
            <div class="row mb-4">
                <div class="col-md-8 mx-auto">
                    <h4><strong>Abstract</strong></h4>
                    <p class="text-justify">
                        While most conversational AI systems focus on textual dialogue only, conditioning utterances on visual context (when it's available) can lead to more realistic conversations.  Unfortunately, a major challenge for incorporating visual context into conversational dialogue is the lack of large-scale labeled datasets. 
We provide a solution in the form of a new visually conditioned Future Utterance Prediction task. Our task involves predicting the next utterance in a video, using both visual frames and transcribed speech as context. By exploiting the large number of instructional videos online, we train a model to solve this task at scale, without the need for manual annotations.
Leveraging recent advances in multimodal learning, our model consists of a novel co-attentional multimodal video transformer, and when trained on both textual and visual context, outperforms baselines that use textual inputs alone. Further, we demonstrate that our model trained for this task on unlabelled videos achieves state-of-the-art performance on a number of downstream VideoQA benchmarks such as MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA.
                    </p>
                </div>
            </div>
            <div class="row mb-4">
                <div class="col-md-8 mx-auto">
                    <h4><strong>Model</strong></h4>
                    <div id="dynamic-teaser">
                        <div id="teaser-dtu" class="pt-3 pb-4">
                            <div class="row no-gutters">
                                <div class="col-12">
                                    <span class="helper"></span><img src="assets/model.png" class="img-responsive">
                                </div>
                            </div> 
                        </div>
                    </div> <!-- dynamic-teaser -->
                    <p class="text-justify">
                        Our goal is to effectively learn from both vision and text in a video. We propose a Co-attentional Multimodal Video Transformer (CoMVT), which given a video clip, extracts contextualized word embeddings and visual features from transcribed words and video frames respectively, and fuses the extracted features to form a multimodal video feature using a co-attentional transformer. We train the network to predict the true next utterance from a set of candidate utterances by minimizing the negative log-likelihood of the true next utterance. Additionally, we also train with a BERT-style masked language modelling loss, which appears to have a regularisation effect.
                    </p>
                </div>
            </div>
            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">
                    <h4><strong>Qualitative Results</strong></h4>
                    <div id="dynamic-teaser">
                        <div id="teaser-dtu" class="pt-3 pb-4">
                            <div class="row no-gutters">
                                <div class="col-7">
                                    <center><strong>Multimodal video inputs</strong></center>
                                </div>
                                <div class="col-1">
                                </div>
                                <div class="col-4">
                                    <strong>Predictions</strong>
                                </div>
                            </div> 
                            <div class="row no-gutters">
                                <div class="col-7">
                                    <span class="helper"></span><img src="assets/ex_1.gif" class="img-responsive ex">
                                </div>
                                <div class="col-1">
                                    <p>&nbsp;</p>
                                    <p>&nbsp;</p>
                                    <p>&nbsp;</p>
                                    <p><center><strong>&nbsp;&nbsp;&nbsp;&nbsp;✓</strong></center></p>
                                </div>
                                <div class="col-4">
                                    <p>&nbsp;</p>
                                    <p class="wrong">It's grown.</p>
                                    <p>Now do we have a finished product?</p>
                                    <p class="correct">That was my <strong>watermelon</strong> had completely ripened weeks ago.</p>
                                    <p>I don't think I would drive a bit long enough to get in there.</p>
                                </div>
                            </div> 
                            <div class="row no-gutters">
                                <div class="col-7">
                                    <span class="helper"></span><img src="assets/ex_2.gif" class="img-responsive ex">
                                </div>
                                <div class="col-1">
                                    <p>&nbsp;</p>
                                    <p>&nbsp;</p>
                                    <p><center><strong>&nbsp;&nbsp;&nbsp;&nbsp;✓</strong></center></p>
                                </div>
                                <div class="col-4">
                                    <p>&nbsp;</p>
                                    <p>We like to eat with bone like quit bone.</p>
                                    <p class="correct">So here are the <strong>carrots</strong>.</p>
                                    <p>You need to see what these things are.</p>
                                    <p class="wrong">They're looking fabulous.</p>
                                </div>
                            </div> 
                            <div class="row no-gutters">
                                <div class="col-7">
                                    <span class="helper"></span><img src="assets/ex_4.gif" class="img-responsive ex">
                                </div>
                                <div class="col-1">
                                    <p>&nbsp;</p>
                                    <p><center><strong>&nbsp;&nbsp;&nbsp;&nbsp;✓</strong></center></p>
                                </div>
                                <div class="col-4">
                                    <p>&nbsp;</p>
                                    <p class="correct">So underneath that is some really really nice <strong>meat</strong>.</p>
                                    <p>It is nice to write down a date on the jar.</p>
                                    <p class="wrong">I guess the skin anyway, so it's ready.</p>
                                    <p>Don't click it.</p>
                                </div>
                            </div> 
                            <div class="row no-gutters">
                                <div class="col-7">
                                    <span class="helper"></span><img src="assets/ex_3.gif" class="img-responsive ex">
                                </div>
                                <div class="col-1">
                                    <p>&nbsp;</p>
                                    <p><center><strong>&nbsp;&nbsp;&nbsp;&nbsp;✓</strong></center></p>
                                </div>
                                <div class="col-4">
                                    <p></br>That's bad.</p>
                                    <p class="correct">But so basically I just spray the <strong>folder</strong> and then I just paste this down and I try to get them in all the exact same position.</p>
                                    <p class="wrong">It doesn't take much time at all.</p>
                                    <p>We need to mount this on our a/c condenser.</p>
                                </div>
                            </div> 
                        </div>
                    </div> <!-- dynamic-teaser -->
                    <p class="text-justify">
                        <strong>Qualitative results on HowToFUP.</strong> 
    On the right, we show the results of the baseline model that uses text inputs only (highlighted in <span class="wrong">red</span>) and our multimodal model (highlighted in <span class="correct">green</span>). The GT utterance has a ✓ next to it. 
    Note how the transcript often contains phrases with subtle indications to visual content, such as `here's another one' (second row) and `should come off right like that' (third row). In many of these cases, the correct future utterance refers to an object which can only be known from the visual context (highlighted in bold). The text only model often selects generics utterances, or those which are referred to specifically in previous dialogue (third row, selected candidate has the word `skin').
                    </p>

                </div>
            </div>
            <div class="row mb-2">
                <div class="col-md-8 mx-auto">
                    <h4 class="mb-3"><strong>Bibtex</strong></h4>
                    <div class="bibtex">@inproceedings{seo2021look,
    title = {{Look Before you Speak: Visually Contextualized Utterances}},
    author = {Seo, Paul Hongsuck and Nagrani, Arsha and Schmid, Cordelia},
    booktitle = {Computer Vision and Pattern Recognition (CVPR)},
    Year = {2021}
}</div>
                </div>
            </div>
            <div class="row mb-2">
                <div class="col-md-8 mx-auto">
                    <p>Contact phseo [AT] google [DOT] com for any queries.</p>
                </div>
            </div>
            <div class="row mb-4" id="license">
                <div class="col-md-8 mx-auto grey-container">
                    <br>
                    <p class="text-justify">
                        Look Before you Speak assets are Copyright 2021 Google LLC, licensed under the CC-BY 4.0 license.
                    </p>
                </div>
            </div>

            
        </div> <!-- container -->
    </body>
</DOCTYPE>